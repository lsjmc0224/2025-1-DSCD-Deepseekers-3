#!/usr/bin/env python
import time
import json
import os
import csv
from tqdm import tqdm
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from collections import OrderedDict
from googleapiclient.discovery import build

API_KEY = None
YOUTUBE = None
# ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLSv1_2)

# ======= ìœ í‹¸ =======

def convert_to_rfc3339(date_str, is_start=True):
    return f"{date_str}T00:00:00Z" if is_start else f"{date_str}T23:59:59Z"

def dedup_video_list(videos):
    return list(OrderedDict((v["videoId"], v) for v in videos).values())

def save_csv(data, filename):
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False, encoding="utf-8-sig")
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {filename}")

# ======= API í˜¸ì¶œ =======

def search_videos(keyword, start_rfc, end_rfc, duration=None):
    videos = []
    page_token = None
    while True:
        request = YOUTUBE.search().list(
            part="snippet",
            q=keyword,
            type="video",
            publishedAfter=start_rfc,
            publishedBefore=end_rfc,
            videoDuration=duration,
            maxResults=50,
            pageToken=page_token
        )
        response = request.execute()
        for item in response.get("items", []):
            videos.append({
                "videoId": item["id"]["videoId"],
                "title": item["snippet"]["title"],
                "publishedAt": item["snippet"]["publishedAt"]
            })
        page_token = response.get("nextPageToken")
        if not page_token:
            break
    return videos

def get_video_details(video_ids):
    details = []
    for i in range(0, len(video_ids), 50):
        batch_ids = video_ids[i:i+50]
        request = YOUTUBE.videos().list(
            part="snippet,statistics",
            id=",".join(batch_ids),
            maxResults=50
        )
        response = request.execute()
        for item in response.get("items", []):
            snippet = item.get("snippet", {})
            stats = item.get("statistics", {})
            details.append({
                "videoId": item["id"],
                "title": snippet.get("title", ""),
                "publishedAt": snippet.get("publishedAt", ""),
                "likeCount": stats.get("likeCount", ""),
                "commentCount": stats.get("commentCount", ""),
                "viewCount": stats.get("viewCount", ""),
                "channelId": snippet.get("channelId", "")
            })
    return details

def get_comments_threadsafe(video_id, max_comments=300):
    youtube = build("youtube", "v3", developerKey=API_KEY)  # ê° ìŠ¤ë ˆë“œë§ˆë‹¤ ìƒì„±
    comments_info = []
    page_token = None

    while True:
        try:
            request = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                maxResults=100,
                pageToken=page_token,
                textFormat="plainText"
            )
            response = request.execute()
        except Exception as e:
            print(f"âŒ ëŒ“ê¸€ ìˆ˜ì§‘ ì‹¤íŒ¨ [{video_id}]: {e}")
            break

        for item in response.get("items", []):
            snippet = item["snippet"]["topLevelComment"]["snippet"]
            comments_info.append({
                "ë‚ ì§œ": snippet["publishedAt"],
                "video_id": video_id,
                "comment_id": item["snippet"]["topLevelComment"]["id"],
                "comment_text": snippet["textDisplay"]
            })
            if len(comments_info) >= max_comments:
                return comments_info

        page_token = response.get("nextPageToken")
        if not page_token:
            break

    return comments_info

def fetch_comments_parallel(video_ids, max_comments_per_video=300, max_workers=4):
    comments_all = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(get_comments_threadsafe, vid, max_comments_per_video) for vid in video_ids]
        for future in tqdm(futures, desc="ëŒ“ê¸€ ë³‘ë ¬ ìˆ˜ì§‘", unit="video"):
            try:
                comments_all.extend(future.result())
            except Exception as e:
                print("âŒ ëŒ“ê¸€ ìˆ˜ì§‘ ì‹¤íŒ¨ (ë³‘ë ¬ ì²˜ë¦¬):", e)
    return comments_all

# ======= MAIN =======

def main():
    global API_KEY, YOUTUBE


    # ì‚¬ìš©ì ì…ë ¥
    keyword = input("ê²€ìƒ‰ í‚¤ì›Œë“œ: ").strip()
    start_date = input("ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD): ").strip()
    end_date = input("ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD): ").strip()
    API_KEY = input("ìœ íŠœë¸Œ API í‚¤: ").strip()
    YOUTUBE = build("youtube", "v3", developerKey=API_KEY)

    start_rfc = convert_to_rfc3339(start_date, True)
    end_rfc = convert_to_rfc3339(end_date, False)

    overall_start = time.time()  # âœ… ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì‹œì‘

    cache_file = f"search_cache_{keyword}_{start_date}_{end_date}.json"

    if os.path.exists(cache_file):
        print("âœ… ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
        with open(cache_file, "r", encoding="utf-8") as f:
            videos_for_meta = json.load(f)
    else:
        print("ğŸ” ë¡±í¼/ìˆí¼ ì˜ìƒ ê²€ìƒ‰ ì¤‘...")

        videos_longform = search_videos(keyword, start_rfc, end_rfc, duration="long")
        for v in videos_longform:
            v["duration_type"] = "long"

        videos_shorts = search_videos(keyword, start_rfc, end_rfc, duration="short")
        for v in videos_shorts:
            v["duration_type"] = "short"

        # í•©ì¹˜ê³  ì¤‘ë³µ ì œê±°
        videos_combined = dedup_video_list(videos_longform + videos_shorts)

        # ì €ì¥ìš© ìºì‹œ
        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(videos_combined, f, indent=2, ensure_ascii=False)

        videos_for_meta = videos_combined

        print(f"âœ… ê²€ìƒ‰ëœ ì˜ìƒ ìˆ˜ (ë¡±+ìˆ): {len(videos_for_meta)}ê°œ")
        search_end = time.time()  # âœ… ê²€ìƒ‰ ì‹œê°„ ì¸¡ì • ì¢…ë£Œ
        print(f"\nê²€ìƒ‰ ì‹œê°„: {search_end - overall_start:.2f}ì´ˆ\n")


    video_ids = [v["videoId"] for v in videos_for_meta]
    if not video_ids:
        print("âŒ ê²€ìƒ‰ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("ğŸ“Š ì˜ìƒ í†µê³„ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
    video_details = get_video_details(video_ids)
    save_csv(video_details, f"{keyword}_metadata_{start_date}_{end_date}.csv")
    print(f"âœ… ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(video_details)}ê°œ")
    details_end = time.time()  # âœ… ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì‹œê°„ ì¸¡ì • ì¢…ë£Œ
    print(f"\në©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì‹œê°„: {details_end - search_end:.2f}ì´ˆ\n")

    print("ğŸ’¬ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ (ë³‘ë ¬)...")
    comments = fetch_comments_parallel(video_ids, max_comments_per_video=300)
    save_csv(comments, f"{keyword}_comments_{start_date}_{end_date}.csv")
    print(f"âœ… ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ: {len(comments)}ê°œ")
    comments_end = time.time()  # âœ… ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œê°„ ì¸¡ì • ì¢…ë£Œ
    print(f"\nëŒ“ê¸€ ìˆ˜ì§‘ ì‹œê°„: {comments_end - details_end:.2f}ì´ˆ\n")

    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ.")
    overall_end = time.time()  # âœ… ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì¢…ë£Œ
    print(f"\nì „ì²´ ì‹¤í–‰ ì‹œê°„: {overall_end - overall_start:.2f}ì´ˆ\n")

if __name__ == "__main__":
    main()
