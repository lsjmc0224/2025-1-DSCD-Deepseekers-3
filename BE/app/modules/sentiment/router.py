from fastapi import APIRouter, Query, HTTPException, Depends
from sqlalchemy.orm import Session
from sqlalchemy import select
from datetime import date, datetime
from collections import Counter, defaultdict

from openai import OpenAI
from app.core.config import settings  # settings Î∂àÎü¨Ïò§Í∏∞

from app.core.db import get_db
from app.models import (
    Keywords, CollectedInstizPosts, CollectedYoutubeComments, CollectedTiktokComments,
    InstizPosts, YoutubeComments, TiktokComments,
    ContentAnalysis, Aspects
)
from . import schemas

SOURCE_MAP = {
    "instiz_posts": "Ïª§ÎÆ§ÎãàÌã∞",
    "youtube_comments": "Ïú†ÌäúÎ∏å",
    "tiktok_comments": "Ìã±ÌÜ°"
}

PLATFORM_TABLES = {
    "instiz": (CollectedInstizPosts, InstizPosts),
    "youtube": (CollectedYoutubeComments, YoutubeComments),
    "tiktok": (CollectedTiktokComments, TiktokComments),
}


def generate_summary(contents: list[str], positive_keywords: list[str], negative_keywords: list[str], product: str) -> str:
    client = OpenAI(api_key=settings.OPENAI_API_KEY)

    reviews_text = "\n".join(contents[:100])
    prompt = f"""
ÎãπÏã†ÏùÄ Í≥†Í∞ù Î¶¨Î∑∞ Î∂ÑÏÑù Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. ÏïÑÎûòÎäî ÏµúÍ∑º ÏùºÏ£ºÏùºÍ∞Ñ '{product}'Ïóê ÎåÄÌïú ÏÇ¨Ïö©Ïûê Î¶¨Î∑∞ ÌÖçÏä§Ìä∏Îì§ÏûÖÎãàÎã§. Ïù¥ Î¶¨Î∑∞Îì§ÏùÑ Ï¢ÖÌï© Î∂ÑÏÑùÌïòÏó¨ Ï†ÑÎ∞òÏ†ÅÏù∏ ÏÜåÎπÑÏûê Î∞òÏùëÏùÑ Í∞ÑÍ≤∞ÌïòÍ≥† ÏûêÏó∞Ïä§ÎüΩÍ≤å ÏöîÏïΩÌï¥ Ï£ºÏÑ∏Ïöî.
---

üî∏ Ï†úÌíà Ïù¥Î¶Ñ: {product}

üî∏ Í∏çÏ†ïÏ†ÅÏúºÎ°ú ÏûêÏ£º Ïñ∏Í∏âÎêú ÌÇ§ÏõåÎìú:
{', '.join(positive_keywords) if positive_keywords else 'ÏóÜÏùå'}

üî∏ Î∂ÄÏ†ïÏ†ÅÏúºÎ°ú ÏûêÏ£º Ïñ∏Í∏âÎêú ÌÇ§ÏõåÎìú:
{', '.join(negative_keywords) if negative_keywords else 'ÏóÜÏùå'}

üî∏ ÏÇ¨Ïö©Ïûê Î¶¨Î∑∞ (ÏµúÎåÄ 100Í∞ú):
{reviews_text}

---

‚úÖ Î∂ÑÏÑù ÏöîÏïΩ (ÏÜåÎπÑÏûêÎì§Ïù¥ Ïù¥ Ï†úÌíàÏùÑ Ïñ¥ÎñªÍ≤å ÌèâÍ∞ÄÌïòÎäîÏßÄ, Ïñ¥Îñ§ Ï†êÏùÑ Ïπ≠Ï∞¨ÌïòÍ≥† Ïñ¥Îñ§ Ï†êÏùÑ ÎπÑÌåêÌñàÎäîÏßÄ Ï§ëÏã¨ÏúºÎ°ú 3Î¨∏Ïû•ÏúºÎ°ú ÏöîÏïΩ ÏûëÏÑ±Ìï¥ Ï£ºÏÑ∏Ïöî):
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ÎÑàÎäî Í≥†Í∞ù Î¶¨Î∑∞ Î∂ÑÏÑù Ï†ÑÎ¨∏Í∞ÄÏïº."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=300
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"[OpenAI Ïò§Î•ò] {e}")
        return "Î¶¨Î∑∞ ÏöîÏïΩÏùÑ ÏÉùÏÑ±ÌïòÎäî Îç∞ Ïã§Ìå®ÌñàÏäµÎãàÎã§."

router = APIRouter()

PLATFORM_TABLES = {
    "instiz": (CollectedInstizPosts, InstizPosts),
    "youtube": (CollectedYoutubeComments, YoutubeComments),
    "tiktok": (CollectedTiktokComments, TiktokComments),
}


@router.get("/overview", response_model=schemas.SentimentOverviewResponse)
async def get_sentiment_overview(
    product: str = Query(...),
    from_: date = Query(..., alias="from"),
    to: date = Query(...),
    db: Session = Depends(get_db)
):
    from_dt = datetime.combine(from_, datetime.min.time())
    to_dt = datetime.combine(to, datetime.max.time())

    # 1. ÌÇ§ÏõåÎìú ID
    keyword_obj = db.execute(select(Keywords).where(Keywords.keyword == product)).scalar_one_or_none()
    if not keyword_obj:
        raise HTTPException(status_code=404, detail="Keyword not found")
    keyword_id = keyword_obj.id

    # 2. Ï†ÑÏ≤¥ ÌîåÎû´ÌèºÏóêÏÑú ÏΩòÌÖêÏ∏† ÏàòÏßë
    source_type_to_ids = defaultdict(list)
    all_contents = []

    for platform, (collected_model, original_model) in PLATFORM_TABLES.items():
        source_type = original_model.__tablename__
        collected_rows = db.execute(
            select(collected_model).where(collected_model.keyword_id == keyword_id)
        ).scalars().all()
        id_field = getattr(collected_model, list(collected_model.__table__.columns)[0].name)
        content_ids = [getattr(row, id_field.name) for row in collected_rows]

        contents = db.execute(
            select(original_model).where(
                original_model.id.in_(content_ids),
                original_model.is_analyzed == True,
                original_model.created_at.between(from_dt, to_dt)
            )
        ).scalars().all()

        all_contents.extend(contents)
        for content in contents:
            source_type_to_ids[source_type].append(str(content.id))

    if not all_contents:
        return schemas.SentimentOverviewResponse(
            summary="Ìï¥Îãπ Í∏∞Í∞Ñ ÎèôÏïà Î∂ÑÏÑùÎêú ÏΩòÌÖêÏ∏†Í∞Ä ÏóÜÏäµÎãàÎã§.",
            positive_keywords=[],
            negative_keywords=[],
            attribute_sentiment=[]
        )


    # 3. Î∂ÑÏÑù Í≤∞Í≥º Ï∂îÏ∂ú
    all_analysis = db.execute(
        select(ContentAnalysis, Aspects.name)
        .join(Aspects, ContentAnalysis.aspect_id == Aspects.id)
        .where(
            ContentAnalysis.source_type.in_(source_type_to_ids.keys()),
            ContentAnalysis.source_id.in_(
                [sid for sids in source_type_to_ids.values() for sid in sids]
            )
        )
    ).all()

    # 4. ÌÇ§ÏõåÎìú Ï∂îÏ∂ú
    positive_counter = Counter()
    negative_counter = Counter()
    attribute_sentiment_map = defaultdict(lambda: {"Í∏çÏ†ï": 0, "Î∂ÄÏ†ï": 0})

    for analysis, aspect_name in all_analysis:
        if analysis.sentiment_id == 1:
            positive_counter[analysis.evidence_keywords] += 1
            attribute_sentiment_map[aspect_name]["Í∏çÏ†ï"] += 1
        elif analysis.sentiment_id == 0:
            negative_counter[analysis.evidence_keywords] += 1
            attribute_sentiment_map[aspect_name]["Î∂ÄÏ†ï"] += 1

    # 5. Í≤∞Í≥º Ï†ïÎ¶¨
    positive_keywords = [kw for kw, _ in positive_counter.most_common(10)]
    negative_keywords = [kw for kw, _ in negative_counter.most_common(10)]

    attribute_sentiment = [
        schemas.AttributeSentimentItem(
            name=aspect,
            Í∏çÏ†ï=counts["Í∏çÏ†ï"],
            Î∂ÄÏ†ï=counts["Î∂ÄÏ†ï"]
        )
        for aspect, counts in attribute_sentiment_map.items()
    ]

    # 6. ÏöîÏïΩ ÏÉùÏÑ±
    sentences = [getattr(c, "text", getattr(c, "content", "")) for c in all_contents]
    summary_text = generate_summary(
        contents=sentences,
        positive_keywords=positive_keywords,
        negative_keywords=negative_keywords,
        product=product
    )

    return schemas.SentimentOverviewResponse(
        summary=summary_text,
        positive_keywords=positive_keywords,
        negative_keywords=negative_keywords,
        attribute_sentiment=attribute_sentiment
    )


@router.get("/details", response_model=schemas.SentimentDetailsResponse)
async def get_sentiment_details(
    product: str = Query(...),
    from_: date = Query(..., alias="from"),
    to: date = Query(...),
    top: int = Query(10),
    db: Session = Depends(get_db)
):
    from_dt = datetime.combine(from_, datetime.min.time())
    to_dt = datetime.combine(to, datetime.max.time())

    # 1. ÌÇ§ÏõåÎìú ÌôïÏù∏
    keyword_obj = db.execute(
        select(Keywords).where(Keywords.keyword == product)
    ).scalar_one_or_none()

    if not keyword_obj:
        raise HTTPException(status_code=404, detail="Keyword not found")

    keyword_id = keyword_obj.id
    content_data = []

    # 2. Î™®Îì† ÌîåÎû´ÌèºÏóêÏÑú ÏàòÏßë ‚Üí Î∂ÑÏÑù Ïó¨Î∂Ä True + Í∏∞Í∞Ñ Ï°∞Í±¥ Ï∂©Ï°±
    for _, (collected_model, original_model) in PLATFORM_TABLES.items():
        source_type = original_model.__tablename__

        collected = db.execute(
            select(collected_model).where(collected_model.keyword_id == keyword_id)
        ).scalars().all()

        id_field = getattr(collected_model, list(collected_model.__table__.columns)[0].name)
        ids = [getattr(row, id_field.name) for row in collected]

        originals = db.execute(
            select(original_model).where(
                original_model.id.in_(ids),
                original_model.is_analyzed == True,
                original_model.created_at.between(from_dt, to_dt)
            )
        ).scalars().all()

        for content in originals:
            content_data.append((source_type, content))

    if not content_data:
        return schemas.SentimentDetailsResponse(
            positive=schemas.SentimentDetailsSection(summary="Ìï¥Îãπ Í∏∞Í∞ÑÏóê Î∂ÑÏÑùÎêú ÎåìÍ∏ÄÏù¥ ÏóÜÏäµÎãàÎã§.", comments=[]),
            negative=schemas.SentimentDetailsSection(summary="Ìï¥Îãπ Í∏∞Í∞ÑÏóê Î∂ÑÏÑùÎêú ÎåìÍ∏ÄÏù¥ ÏóÜÏäµÎãàÎã§.", comments=[])
        )

    # 3. Î∂ÑÏÑù Í≤∞Í≥º Ïó∞Í≤∞
    id_grouped = defaultdict(list)
    for source_type, content in content_data:
        id_grouped[source_type].append(str(content.id))

    analysis_map = defaultdict(list)
    for source_type, ids in id_grouped.items():
        analyses = db.execute(
            select(ContentAnalysis).where(
                ContentAnalysis.source_type == source_type,
                ContentAnalysis.source_id.in_(ids)
            )
        ).scalars().all()
        for a in analyses:
            analysis_map[(a.source_type, a.source_id)].append(a)

    # 4. likes Í∏∞Ï§Ä ÏÉÅÏúÑ top% ÌïÑÌÑ∞ÎßÅ
    scored_data = []
    for source_type, content in content_data:
        likes = getattr(content, "like_count", None)
        if likes is not None:
            scored_data.append((source_type, content, likes))

    if not scored_data:
        raise HTTPException(status_code=400, detail="likes Í∞íÏù¥ ÏóÜÎäî Îç∞Ïù¥ÌÑ∞ÏûÖÎãàÎã§.")

    # ÏÉÅÏúÑ top%
    scored_data.sort(key=lambda x: x[2], reverse=True)
    cutoff = max(1, int(len(scored_data) * top / 100))
    top_data = scored_data[:cutoff]

    positive_group, negative_group = [], []

    for source_type, content, likes in top_data:
        key = (source_type, str(content.id))
        if key in analysis_map:
            sentiment_scores = [a.sentiment_id for a in analysis_map[key]]
            avg_score = sum(sentiment_scores) / len(sentiment_scores)
            if avg_score == 1.0:
                positive_group.append((source_type, content))
            elif avg_score == 0.0:
                negative_group.append((source_type, content))

    # 5. ÏµúÏã† 5Í∞ú ÎåìÍ∏Ä
    def build_comments(group: list, label: str) -> list[schemas.CommentItem]:
        sorted_group = sorted(group, key=lambda x: x[1].created_at, reverse=True)
        return [
            schemas.CommentItem(
                id=f"{source_type}-{content.id}",
                text=getattr(content, "text", getattr(content, "content", "")),
                date=content.created_at,
                sentiment=label,
                source=SOURCE_MAP.get(source_type, source_type),
                likes=getattr(content, "like_count", 0)
            )
            for source_type, content in sorted_group[:5]
        ]

    pos_comments = build_comments(positive_group, "positive")
    neg_comments = build_comments(negative_group, "negative")

    # 6. ÏöîÏïΩ ÏÉùÏÑ±
    pos_texts = [c.text for c in pos_comments]
    neg_texts = [c.text for c in neg_comments]

    pos_summary = generate_summary(contents=pos_texts, positive_keywords=[], negative_keywords=[], product=product)
    neg_summary = generate_summary(contents=neg_texts, positive_keywords=[], negative_keywords=[], product=product)

    return schemas.SentimentDetailsResponse(
        positive=schemas.SentimentDetailsSection(summary=pos_summary, comments=pos_comments),
        negative=schemas.SentimentDetailsSection(summary=neg_summary, comments=neg_comments)
    )